---
title: "Webscrape Hawaii Coastal Sites and Characteristics Data"
author: "Alemarie Ceria"
output: 
  html_document:
    toc: true
---

<style type="text/css">

h1.title {
  font-size: 38px;
  color: Black;
  font-weight:bold;
  text-align: center;
}
h4.author { /* Header 4 - and the author and data headers use this too  */
    font-size: 18px;
  color: Black;
  text-align: center;
}
h4.date { /* Header 4 - and the author and data headers use this too  */
  font-size: 18px;
  color: Black;
  text-align: center;
}
</style>

***

# Web scrape [To-Hawaii.com](https://www.to-hawaii.com/maui/beaches/) blog

## Libraries

```{r message=FALSE, warning=FALSE}
# For tidying data
library(tidyverse)

# Used to get /robots.txt file that gives instructions about websites to web robots
library(robotstxt)

# Web scraping packages
library(rvest)
library(RSelenium)

# For parsing json documents
library(jsonlite)
```

## Webscraping

```{r}
rs_driver_object <- rsDriver(browser = "chrome",
                             port = 9515L,
                             chromever = "109.0.5414.74")

obj <- rdriver$client

obj$navigate("https://www.to-hawaii.com")

obj$close
```


## Tutorial

### Web scraping instructions

"User-agent: *
Disallow: /example_path/"

Meaning: Any robot can go wherever, but just don't go to what is specified under "disallow"

"User-agent: Google
Disallow:"

Meaning: Google's robot can go wherever

"User-agent: *
Disallow: /"

Meaning: All robots are not allowed to go anywhere

```{r Get Website Robot Text File, message=FALSE, warning=FALSE}
to_hawaii <- robotstxt('https://www.to-hawaii.com/')
to_hawaii$permissions

look_into_hawaii <- robotstxt('https://lookintohawaii.com/')
look_into_hawaii$permissions

# Get robots.txt file
# get_robotstxt() 

# Check a specific URL (TRUE/FALSE)
# paths_allowed()

# Check multiple paths
# paths_allowed(
#   paths = c(''),
#   domain = c(''),
#   bot = '*'
# ) 
```

### Craw delays

```{r Crawl Delays, message=FALSE, warning=FALSE}
# Check website's recommended crawl delay, but standard is 5 - 10 seconds between calls
# object_name$crawl_delay

# Hanjo Odendaal's nytnyt function
# nytnyt <- function(periods = c(5, 10)) { # Period 1 (5 secs) and period 2 (10 secs)
#   # Generate a random number between period 1 and period 2
#   tictoc <- runif(1, periods[1], periods[2])
#   # Output text
#   cat(paste0(Sys.time()), '- Sleeping for ', round(tictoc, 2), 'seconds\n')
#   # Sleep
#   Sys.sleep(tictoc)
# }

# Test
# for(i in 1:5) {
#   cat('Hello website', i, '\n')
#   nytnyt(periods = c(5, 10))
# }

# nytnyt() example output: "Hello website 1 2023-01-19 20:36:18 - Sleeping for  9.77 seconds"
```

### Get IP function

```{r Get IP, message=FALSE, warning=FALSE}
get_ip <- function(){
  read_html('https://api.ipify.org/?format=json') %>% 
    html_text(., trim = TRUE) %>% 
    jsonlite::fromJSON(.)
}

get_ip()
```

### Document Object Model (DOM)

![](G:/.shortcut-targets-by-id/1xsGdAg4Wshwmh3vEnWe-mxuOrR85IfQ4/Coastal ecosystem accounting/1c. Ecosystem use/hawaii_tcrum/visualizations/dom_example.png)

- A programming interface (API) for HTML and XML documents
- Represents documents as nodes and objects
- Object-oriented representation of web page, which can be modified or accessed

```{r DOM Example, message=FALSE, warning=FALSE}
ahihi_cove <- read_html('https://www.to-hawaii.com/maui/beaches/ahihicove.php')

ahihi_table <- ahihi_cove %>% html_nodes(c('[id*=contenttab]'))
```

Tip: Keep webpages as separate objects. Do not pipe html_node(s) in case of mistakes.

### URL breakdown:

- ?: Breaks up the API and parameters follow
- &: Breaks up parameters
- "start=": Incremental control of the pages

## Resources:

1. The ultimate online collection toolbox: Combining RSelenium and Rvest
     - [Part 1](https://www.youtube.com/watch?v=OxbvFiYxEzI) 
     - [Part 2](https://www.youtube.com/watch?v=JcIeWiljQG4)
2. [Test your IP](https://api.ipify.org/?format=json)
3. [DOM tree document](https://javascript.info/dom-nodes)
4. [Css Selector Reference](https://www.w3schools.com/cssref/css_selectors.php)
5. [How to Web Scrape RateMyProfessors Using RSelenium](https://www.youtube.com/watch?v=mWUOdV2nMOk)

***

# Google Maps Places API - Nearby Search Request